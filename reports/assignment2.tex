\documentclass{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{geometry}
\geometry{margin=1in}

\title{Assignment 2 - CIFAR-10 Classification with 2-layer Networks}
\author{Martín Bravo, DD2424 Deep Learning, KTH Royal Institute of Technology}
\date{\today}

\begin{document}

\maketitle

% In this assignment you will train and test a one layer network with multi-
% ple outputs to classify images from the CIFAR-10 dataset. You will train
% the network using mini-batch gradient descent applied to a cost function
% that computes the cross-entropy loss of the classifier applied to the labelled
% training data and an L2 regularization term on the weight matrix.

% Background 1: Mathematical background
% The mathematical details of the network are as follows. Given an input
% vector, x, of size d × 1 our classifier outputs a vector of probabilities, p
% (K × 1), for each possible output label:
% s = W x + b (1)
% p = SOFTMAX(s) (2)
% where the matrix W has size K × d, the vector b is K × 1 and SOFTMAX is
% defined as
% SOFTMAX(s) = exp(s)
% 1T exp(s) (3)
% The predicted class corresponds to the label with the highest probability:
% k∗ = arg max
% 1≤k≤K {p1, . . . , pK } (4)x z s p
% W b
% W x z + b softmax(s)x z s p l
% W b y
% W x z + b softmax(s) −yT log(p)
% a) Classification function b) Loss function
% Figure 1: Computational graph of the classification and loss function that is ap-
% plied to each input x in this assignment.
% The classifier’s parameters W and b are what we have to learn from the
% labelled training data. Let D = {(xi, yi)}n
% i=1, with each yi ∈ {1, . . . , K}
% and xi ∈ Rd, represent our labelled training data. In the lectures we have
% described how to set the parameters by minimizing the cross-entropy loss
% plus a regularization term on W . Mathematically this cost function is
% J(D, λ, W, b) = 1
% |D|
% X
% (x,y)∈D
% lcross(x, y, W, b) + λ X
% i,j
% W 2
% ij (5)
% 1
% where
% lcross(x, y, W, b) = − log(py) (6)
% and p has been calculated using equations (1, 2). (Note if the label is
% encoded as one-hot representation then the cross-entropy loss is defined as
% −yT log(p) = log(py).) The optimization problem we have to solve is
% W ∗, b∗ = arg min
% W,b J(D, λ, W, b) (7)
% In this assignment (as described in the lectures) we will solve this optimiza-
% tion problem via mini-batch gradient descent.
% For mini-batch gradient descent we begin with a sensible random initial-
% ization of the parameters W, b and we then update our estimate for the
% parameters with
% W (t+1) = W (t) − η ∂J(B(t+1), λ, W, b)
% ∂W W =W (t),b=b(t)
% (8)
% b(t+1) = b(t) − η ∂J(B(t+1), λ, W, b)
% ∂b W =W (t),b=b(t)
% (9)
% where η is the learning rate and B(t+1) is called a mini-batch and is a random
% subset of the training data D and
% ∂J(B(t+1), λ, W, b)
% ∂W = 1
% |B(t+1)|
% X
% (x,y)∈B(t+1)
% ∂lcross(x, y, W, b)
% ∂W + 2λW (10)
% ∂J(B(t+1), λ, W, b)
% ∂b = 1
% |B(t+1)|
% X
% (x,y)∈B(t+1)
% ∂lcross(x, y, W, b)
% ∂b (11)
% To compute the relevant gradients for the mini-batch, we then have to com-
% pute the gradient of the loss w.r.t. each training example in the mini-batch.
% You should refer to the lecture notes for the explicit description of how to
% compute these gradients.

\section{Introduction}
This assignment explores the implementation and optimization of a 2-layer neural network for image classification on the CIFAR-10 dataset. We investigate several key aspects of neural network training, including gradient computation, cyclic learning rates, regularization techniques, and optimization methods.

The main objectives are to:
\begin{itemize}
    \item Implement and verify gradient computations for backpropagation
    \item Study the effects of different hyperparameters like learning rate and regularization strength
    \item Improve model performance through techniques like dropout, data augmentation, and Adam optimization
\end{itemize}

Through these experiments, we demonstrate how various components work together to create an effective neural network classifier, achieving improved accuracy while maintaining computational efficiency.



\section{Dataset and Preprocessing}
The CIFAR-10 dataset consists of $32\times32$ color images belonging to 10 classes. We normalized the input data per feature using the training set mean and standard deviation. The dataset was split into training, validation, and test sets. Below are a few CIFAR-10 examples:

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.6\textwidth]{imgs/assignment1_cifar_examples.png}
    \caption{Sample CIFAR-10 images from the dataset}
\end{figure}

\section{Background}

\subsection{2-layer Network}
The architecture of the network is a 2-layer network. This is one of the simplest forms of neural networks, often referred to as a Multi-Layer Perceptron (MLP). The input to the network is a vectorized image of size $d = 32 \times 32 \times 3 = 3072$, where each image is flattened into a one-dimensional vector. The network consists of the following components:

\begin{itemize}
    \item A weight matrix $W_1$ of size $3072 \times m$, where $m$ is the number of hidden units.
    \item A bias vector $b_1$ of size $m \times 1$.
    \item A weight matrix $W_2$ of size $m \times 10$, where $10$ is the number of classes.
    \item A bias vector $b_2$ of size $10 \times 1$.
\end{itemize}

To compute the output of the network, we first compute the scores for each class as:
\[s_1 = W_1 x + b_1, \quad z = \text{ReLU}(s_1), \quad s_2 = W_2 z + b_2, \quad p = \text{Softmax}(s_2)\]

The predicted class is then given by:
\[
k^* = \arg\max_{1 \leq k \leq 10} p_k
\]

This allows more complexity compared to the simple linear classifier, as the hidden layer allows for non-linear transformations of the input.

\subsection{Cyclic Learning Rate}

The cyclic learning rate is a technique that varies the learning rate during training. It starts at a low value, increases linearly to a high value, and then decreases linearly back to the low value. This is done in a cyclic manner, hence the name.

We use a linear increase and decrease of the learning rate, with a period of $T$ iterations, and a step size of $n_s$. The learning rate is given by:

\begin{equation}
    \eta(t) = \eta_{min} + \frac{t}{n_s} (\eta_{max} - \eta_{min})
\end{equation}

where $t$ is the current iteration, $T$ is the total number of iterations, $n_s$ is the step size, $\eta_{min}$ is the minimum learning rate, and $\eta_{max}$ is the maximum learning rate.


\subsection{Dropout}

Dropout is a regularization technique that randomly drops out units (neurons) during training. This helps prevent overfitting by forcing the network to learn more robust and diverse features.

\subsection{Adam Optimizer}

Adam is an optimizer that uses the first and second moments of the gradient to update the parameters. It is a popular optimizer for deep learning, and is known to converge faster than other optimizers such as SGD.


\subsection{Checking gradients computation}

In order to check the correctness of the gradients computation, we compared our implementation with the one provided by PyTorch. To measure how close the gradients are, we computed the relative error of the gradients:

\begin{equation}
    \epsilon = \frac{\| \nabla J - \nabla J_{\text{PyTorch}} \|}{\| \nabla J \| + \| \nabla J_{\text{PyTorch}} \|} \leq \delta
\end{equation}

We test that the distance between the gradients is less than a constant value $\delta$.

\newpage

\section{Results}
We conducted four main experiments, we first get the results of figure 3 and 4 from the assignment description, then we find the optimal $\lambda$ for the network, then we improve the results with more hidden units, dropout, and data augmentation. Finally, we use the Adam optimizer to see if it improves the results.

\subsection{Experiment 1: Replicating the results for 1 cycle and 3 cycles}

We run the network with the parameters specified in the assignment description and get the results shown in Figure \ref{fig:experiment1} and Figure \ref{fig:experiment2}, we compare the accuracy, loss, and cost on the testing set in Table \ref{tab:experiment1}.

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/assignment_2_accuracy_results_e3.png}
        \caption{Accuracy}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/assignment_2_loss_results_e3.png}
        \caption{Loss}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/assignment_2_cost_results_e3.png}
        \caption{Cost}
    \end{subfigure}
    \caption{Networks parameters: $m = 64$, $\lambda = 0.01$, $\eta_{min} = 1e-5$, $\eta_{max} = 1e-1$, $n_s = 500$, $cycles = 1$, batch\_size = 100}
    \label{fig:experiment1}
\end{figure}

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/assignment_2_accuracy_results_e4.png}
        \caption{Accuracy}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/assignment_2_loss_results_e4.png}
        \caption{Loss}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/assignment_2_cost_results_e4.png}
        \caption{Cost}
    \end{subfigure}
    \caption{Networks parameters: $m = 64$, $\lambda = 0.01$, $\eta_{min} = 1e-5$, $\eta_{max} = 1e-1$, $n_s = 800$, $cycles = 3$, batch\_size = 100}
    \label{fig:experiment2}
\end{figure}

\newpage

% --------- Exercise 3 ---------
% Training with 1 cycle
% Training setup: 10000 training samples, 3072 features, 1 cycles, 500 step size, 100 batch size, 0.0 dropout rate, False use Adam optimizer
% Epochs: 100%|█████████████| 10/10 [00:13<00:00,  1.37s/it]
% Network performs with accuracy: 0.4731, loss: 1.5159828323591822, and cost: 1.797066046434232
% --------------------------------
% --------------------------------
% Running Train network from figure 4
% --------- Exercise 4 ---------
% Training with 3 cycles
% Training setup: 10000 training samples, 3072 features, 3 cycles, 800 step size, 100 batch size, 0.0 dropout rate, False use Adam optimizer
% Epochs: 100%|█████████████| 48/48 [00:53<00:00,  1.11s/it]
% Network performs with accuracy: 0.4798, loss: 1.5034834341841317, and cost: 1.8432568886743304

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{Experiment} & \textbf{Accuracy} & \textbf{Loss} & \textbf{Cost} \\
    \hline
    \textbf{1 cycle} & 0.4731 & 1.5159 & 1.7970 \\
    \textbf{3 cycles} & 0.4798 & 1.5034 & 1.8432 \\
    \hline
\end{tabular}
\caption{Comparing the accuracy, loss, and cost on the testing set for 1 cycle and 3 cycles networks}
\label{tab:experiment1}
\end{table}

We can see in Table \ref{tab:experiment1} that the 3 cycles network performs better than the 1 cycle network, but not by much. This improvement is expected, since if we use 3 cycles of the learning rate, each cycle will imply more epochs where the model is trained. We can appreciate that in the last cycles, the plots experience divergence, but this is fixed at the end of the cycle, generally achieving better results than the past cycles, this is due to the nature of the cyclic learning rate.


\subsection{Experiment 2: Finding the optimal $\lambda$}

For the next experiment, the goal is to find the optimal $\lambda$ for the network. We run the network with different values of $\lambda$, we sample them by using the formula:

\begin{equation}
    \log_{10}(\lambda) = \log_{10}(\lambda_{min}) + \text{random(0, 1)} (\log_{10}(\lambda_{max}) - \log_{10}(\lambda_{min}))
\end{equation}

where our $\log_{10}(\lambda_{min}) = -5$ and $\log_{10}(\lambda_{max}) = -1$. We run the network with these values and get the results shown in Table \ref{tab:experiment2}.


\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{$\lambda$} & \textbf{Train Accuracy} & \textbf{Validation Accuracy} & \textbf{Test Accuracy} \\
    \hline
    \textbf{0.0011} & \textbf{0.5637} & \textbf{0.5116} & \textbf{0.5142} \\
    0.0000215 & 0.5670 & 0.5194 & 0.5138 \\
    0.0078 & 0.5404 & 0.5122 & 0.5088 \\
    0.000081 & 0.5662 & 0.5216 & 0.5083 \\
    0.0007 & 0.5648 & 0.5182 & 0.5081 \\
    0.0000176 & 0.5632 & 0.5156 & 0.5064 \\
    0.0000198 & 0.5695 & 0.5138 & 0.5053 \\
    0.0167 & 0.5095 & 0.4916 & 0.4945 \\
    0.0204 & 0.5004 & 0.4856 & 0.4845 \\
    0.0949 & 0.3912 & 0.3788 & 0.3918 \\
    \hline
    \end{tabular}
    \caption{Model performance for different values of $\lambda$. The best test accuracy is highlighted in \textbf{bold}.}
    \label{tab:experiment2}
\end{table}

We estimate that the optimal $\lambda \approx 0.001$, for this we run the network and get the results shown in the figure below.

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/assignment_2_accuracy_results_e4_optimal_lambda.png}
        \caption{Accuracy}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/assignment_2_loss_results_e4_optimal_lambda.png}
        \caption{Loss}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{imgs/assignment_2_cost_results_e4_optimal_lambda.png}
        \caption{Cost}
    \end{subfigure}
    \caption{Networks parameters: $m = 64$, $\lambda = 0.001$, $\eta_{min} = 1e-5$, $\eta_{max} = 1e-1$, $n_s = 800$, $cycles = 3$, batch\_size = 100}
    \label{fig:experiment2_optimal_lambda}
\end{figure}

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{Experiment} & \textbf{Accuracy} & \textbf{Loss} & \textbf{Cost} \\
    \hline
    \textbf{1 cycle} & 0.4731 & 1.5159 & 1.7970 \\
    \textbf{3 cycles} & 0.4798 & 1.5034 & 1.8432 \\
    \textbf{Optimal $\lambda$} & 0.5142 & 1.4945 & 1.7969 \\
    \hline
    \end{tabular}
    \caption{Comparing the accuracy, loss, and cost on the testing set for 1 cycle and 3 cycles networks}
    \label{tab:experiment2_optimal_lambda}
\end{table}

We can see in Table \ref{tab:experiment2_optimal_lambda} that using this value of $\lambda$ we get the best results, we can see that the accuracy increases by almost 4\% compared to the previous best results. Analyzing the loss and cost plots in Figure \ref{fig:experiment2_optimal_lambda}, we can see that the training is more stable, we still have the divergence caused by the cyclic learning rate, but it is not as severe as before. Improving the regularization of the network made the learning smoother and more stable.

\newpage

\subsection{Experiment 3: Improving results with more hidden units, dropout, and data augmentation}

% --------------------------------
% Running Improve results with more hidden units, dropout, and data augmentation
% --------- Exercise 5 ---------
% Improving results, with more hidden units, dropout, and data augmentation
% Training setup: 90000 training samples, 3072 features, 5 cycles, 900 step size, 100 batch size, 0.2 dropout rate, False use Adam optimizer
% Epochs: 100%|████████████| 10/10 [26:51<00:00, 161.13s/it]
% Accuracy: 0.5311, Loss: 1.3546671361649176, Cost: 1.532303311974826

% def data_augmentation(X):
%     X_aug = X.copy()
%     N = X.shape[1]

%     for i in range(N):
%         img = X_aug[:, i].reshape(3, 32, 32).transpose(1, 2, 0)  # (32, 32, 3)

%         if np.random.rand() > 0.5:
%             img = np.fliplr(img)

%         tx = np.random.randint(-3, 4)
%         ty = np.random.randint(-3, 4)
%         M_trans = np.float32([[1, 0, tx], [0, 1, ty]])
%         for c in range(3):
%             img[:, :, c] = cv2.warpAffine(
%                 img[:, :, c], M_trans, (32, 32), borderMode=cv2.BORDER_REFLECT
%             )

%         angle = np.random.uniform(-15, 15)
%         M_rot = cv2.getRotationMatrix2D((16, 16), angle, 1)
%         for c in range(3):
%             img[:, :, c] = cv2.warpAffine(
%                 img[:, :, c], M_rot, (32, 32), borderMode=cv2.BORDER_REFLECT
%             )

%         X_aug[:, i] = img.transpose(2, 0, 1).reshape(-1)

%     return X_aug

For the following experiment, we train the network with more hidden units ($m = 512$), dropout ($p = 0.2$), and data augmentation (where we randomly flip the images, translate them, and rotate them). We get the results shown in Table \ref{tab:experiment3}.

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{Experiment} & \textbf{Accuracy} & \textbf{Loss} & \textbf{Cost} \\
    \hline
    \textbf{1 cycle} & 0.4731 & 1.5159 & 1.7970 \\
    \textbf{3 cycles} & 0.4798 & 1.5034 & 1.8432 \\
    \textbf{Hidden units + dropout + data augmentation (3 cycles)} & 0.5311 & 1.3546 & 1.5323 \\
    \hline
    \end{tabular}
    \caption{Comparing the accuracy, loss, and cost on the testing set for 1 cycle, 3 cycles, and hidden units + dropout + data augmentation networks}
    \label{tab:experiment3}
\end{table}

We can see that the accuracy increases by 6\% compared to the previous best results, making this network the best performing one so far. The hidden units allows the model to learn more complex features, the dropout helps prevent overfitting, and the data augmentation helps the model generalize better.


\subsection{Experiment 4: Using Adam optimizer}


% --------------------------------
% Running Improve results with Adam optimizer
% --------- Exercise 6 ---------
% Improving results with Adam Optimizer
% Training setup: 10000 training samples, 3072 features, 3 cycles, 800 step size, 100 batch size, 0.0 dropout rate, True use Adam optimizer
% Epochs: 100%|█████████████| 48/48 [01:25<00:00,  1.78s/it]
% Accuracy: 0.4754, Loss: 1.497265002251512, Cost: 1.7969284385458661
% --------------------------------

We train the network with the Adam optimizer and get the results shown in Table \ref{tab:experiment4}.

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{Experiment} & \textbf{Accuracy} & \textbf{Loss} & \textbf{Cost} \\
    \hline
    \textbf{1 cycle} & 0.4731 & 1.5159 & 1.7970 \\
    \textbf{Adam optimizer (1 cycle)} & 0.4754 & 1.4972 & 1.7969 \\
    \hline
    \end{tabular}
    \caption{Comparing the accuracy, loss, and cost on the testing set for 1 cycle, and Adam optimizer networks}
    \label{tab:experiment4}
\end{table}

We can see in Table \ref{tab:experiment4} that using the Adam optimizer we get slightly better results than the 1 cycles network, but not by much. This is due to the fact that the Adam optimizer makes the learning faster due to the momentum.


\newpage

\section{Conclusion}
In this assignment, we implemented a two-layer neural network for image classification on the CIFAR-10 dataset. We explored several techniques to improve the network's performance:

\begin{itemize}
    \item Using multiple cycles in the learning rate schedule improved accuracy from 47.31\% to 47.98\%
    \item Adding more hidden units, dropout, and data augmentation further increased accuracy to 53.11\%
    \item The Adam optimizer provided marginal improvements over standard SGD
\end{itemize}

The best performing model achieved 53.11\% accuracy on the test set, demonstrating the effectiveness of combining multiple optimization techniques. While this is a significant improvement over the baseline, there is still room for better performance through techniques like deeper architectures, batch normalization, or more sophisticated data augmentation.



\end{document}


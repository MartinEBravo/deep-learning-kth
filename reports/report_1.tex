\documentclass{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{geometry}
\geometry{margin=1in}

\title{Assignment 1 - CIFAR-10 Classification}
\author{Martín Bravo}
\date{\today}

\begin{document}

\maketitle

In this assignment you will train and test a one layer network with multi-
ple outputs to classify images from the CIFAR-10 dataset. You will train
the network using mini-batch gradient descent applied to a cost function
that computes the cross-entropy loss of the classifier applied to the labelled
training data and an L2 regularization term on the weight matrix.

% Background 1: Mathematical background
% The mathematical details of the network are as follows. Given an input
% vector, x, of size d × 1 our classifier outputs a vector of probabilities, p
% (K × 1), for each possible output label:
% s = W x + b (1)
% p = SOFTMAX(s) (2)
% where the matrix W has size K × d, the vector b is K × 1 and SOFTMAX is
% defined as
% SOFTMAX(s) = exp(s)
% 1T exp(s) (3)
% The predicted class corresponds to the label with the highest probability:
% k∗ = arg max
% 1≤k≤K {p1, . . . , pK } (4)x z s p
% W b
% W x z + b softmax(s)x z s p l
% W b y
% W x z + b softmax(s) −yT log(p)
% a) Classification function b) Loss function
% Figure 1: Computational graph of the classification and loss function that is ap-
% plied to each input x in this assignment.
% The classifier’s parameters W and b are what we have to learn from the
% labelled training data. Let D = {(xi, yi)}n
% i=1, with each yi ∈ {1, . . . , K}
% and xi ∈ Rd, represent our labelled training data. In the lectures we have
% described how to set the parameters by minimizing the cross-entropy loss
% plus a regularization term on W . Mathematically this cost function is
% J(D, λ, W, b) = 1
% |D|
% X
% (x,y)∈D
% lcross(x, y, W, b) + λ X
% i,j
% W 2
% ij (5)
% 1
% where
% lcross(x, y, W, b) = − log(py) (6)
% and p has been calculated using equations (1, 2). (Note if the label is
% encoded as one-hot representation then the cross-entropy loss is defined as
% −yT log(p) = log(py).) The optimization problem we have to solve is
% W ∗, b∗ = arg min
% W,b J(D, λ, W, b) (7)
% In this assignment (as described in the lectures) we will solve this optimiza-
% tion problem via mini-batch gradient descent.
% For mini-batch gradient descent we begin with a sensible random initial-
% ization of the parameters W, b and we then update our estimate for the
% parameters with
% W (t+1) = W (t) − η ∂J(B(t+1), λ, W, b)
% ∂W W =W (t),b=b(t)
% (8)
% b(t+1) = b(t) − η ∂J(B(t+1), λ, W, b)
% ∂b W =W (t),b=b(t)
% (9)
% where η is the learning rate and B(t+1) is called a mini-batch and is a random
% subset of the training data D and
% ∂J(B(t+1), λ, W, b)
% ∂W = 1
% |B(t+1)|
% X
% (x,y)∈B(t+1)
% ∂lcross(x, y, W, b)
% ∂W + 2λW (10)
% ∂J(B(t+1), λ, W, b)
% ∂b = 1
% |B(t+1)|
% X
% (x,y)∈B(t+1)
% ∂lcross(x, y, W, b)
% ∂b (11)
% To compute the relevant gradients for the mini-batch, we then have to com-
% pute the gradient of the loss w.r.t. each training example in the mini-batch.
% You should refer to the lecture notes for the explicit description of how to
% compute these gradients.

\section{Introduction}
In this assignment, we implemented a simple classifier for the CIFAR-10 dataset using softmax regression (multinomial logistic regression). We explored the effects of different hyperparameters, such as learning rate ($\eta$), number of epochs, batch size, and L2 regularization strength ($\lambda$). Additionally, we visualized the learned weights as images and evaluated training, validation, and test performance.

\section{Dataset and Preprocessing}
The CIFAR-10 dataset consists of $32\times32$ color images belonging to 10 classes. We normalized the input data per feature using the training set mean and standard deviation. The dataset was split into training, validation, and test sets. Below are a few CIFAR-10 examples:

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.6\textwidth]{imgs/assignment1_cifar_examples.png}
    \caption{Sample CIFAR-10 images from the dataset}
\end{figure}
\section{Architecture}
The architecture of the network is a single-layer softmax regression model. This is one of the simplest forms of neural networks, often referred to as a generalized linear model for classification. The input to the network is a vectorized image of size $d = 32 \times 32 \times 3 = 3072$, where each image is flattened into a one-dimensional vector. The network consists of the following components:

\begin{itemize}
    \item A weight matrix $W$ of size $K \times d$, where $K=10$ is the number of classes. Each row of $W$ corresponds to the weights associated with a specific class.
    \item A bias vector $b$ of size $K \times 1$, which allows the model to shift the decision boundaries independently of the input features.
\end{itemize}

The forward pass of the network computes the scores for each class as:
\[
s = W x + b
\]
where $x$ is the input vector. These scores, $s$, are then transformed into probabilities using the softmax function:
\[
p = \text{SOFTMAX}(s) = \frac{\exp(s)}{\mathbf{1}^T \exp(s)}
\]
Here, $\exp(s)$ is applied element-wise, and the denominator ensures that the probabilities sum to 1. This normalization step is crucial for interpreting the output as probabilities.

The predicted class is determined by selecting the class with the highest probability:
\[
k^* = \arg\max_{1 \leq k \leq K} p_k
\]

This simple architecture allows us to directly map input features to class probabilities using a linear transformation followed by the softmax activation. Despite its simplicity, this model is effective for linearly separable data and serves as a strong baseline for more complex architectures.

The softmax regression model is particularly well-suited for multi-class classification problems, as it generalizes logistic regression to handle multiple classes. However, its performance is limited when the data is not linearly separable, which is why more advanced architectures, such as multi-layer neural networks, are often used in practice. In this assignment, we focus on understanding the behavior of this simple model and its sensitivity to hyperparameters like the learning rate ($\eta$) and regularization strength ($\lambda$).

\section{Experiments}
We conducted four main experiments with varying values of $\eta$ and $\lambda$ while keeping $n_{epochs}=40$ and $batch=100$.

\subsection{Effect of the Learning Rate and the Regularization Strength}

Figure~\ref{fig:loss_experiments} shows the evolution of training and validation loss for different configurations of learning rate ($\eta$) and L2 regularization strength ($\lambda$). Table~\ref{tab:experiment_results} summarizes the final accuracy on train, validation, and test sets for each experiment.

\begin{figure}[h!]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\linewidth]{imgs/assignment1_loss_evolution_experiment_1.png}
        \caption{$\eta=0.1$, $\lambda=0$}
        \label{fig:exp1}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\linewidth]{imgs/assignment1_loss_evolution_experiment_2.png}
        \caption{$\eta=0.001$, $\lambda=0$}
        \label{fig:exp2}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\linewidth]{imgs/assignment1_loss_evolution_experiment_3.png}
        \caption{$\eta=0.001$, $\lambda=0.1$}
        \label{fig:exp3}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\linewidth]{imgs/assignment1_loss_evolution_experiment_4.png}
        \caption{$\eta=0.001$, $\lambda=1$}
        \label{fig:exp4}
    \end{subfigure}
    \caption{Training and validation loss curves for different hyperparameter configurations.}
    \label{fig:loss_experiments}
\end{figure}

\begin{table}[h!]
    \centering
    \caption{Final accuracy (\%) for each experiment configuration.}
    \label{tab:experiment_results}
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        \textbf{Experiment} & \boldmath$\eta$ & \boldmath$\lambda$ & \textbf{Train Acc.} & \textbf{Val/Test Acc.} \\
        \hline
        1 & 0.1   & 0   & 42.95\% & 26.32\% / 27.00\% \\
        2 & 0.001 & 0   & 45.57\% & 38.46\% / 39.21\% \\
        3 & 0.001 & 0.1 & 44.63\% & 38.62\% / 39.30\% \\
        4 & 0.001 & 1   & 39.85\% & 36.32\% / 37.55\% \\
        \hline
    \end{tabular}
\end{table}

\begin{figure}[h!]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\linewidth]{imgs/assignment1_learned_matrices_experiment_1.png}
        \caption{$\eta=0.1$, $\lambda=0$}
        \label{fig:exp_matrix1}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\linewidth]{imgs/assignment1_learned_matrices_experiment_2.png}
        \caption{$\eta=0.001$, $\lambda=0$}
        \label{fig:exp_matrix2}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\linewidth]{imgs/assignment1_learned_matrices_experiment_3.png}
        \caption{$\eta=0.001$, $\lambda=0.1$}
        \label{fig:exp_matrix3}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\linewidth]{imgs/assignment1_learned_matrices_experiment_4.png}
        \caption{$\eta=0.001$, $\lambda=1$}
        \label{fig:exp_matrix4}
    \end{subfigure}
    \caption{Visualization of the learned weight matrices for each experiment. Each matrix is reshaped into a $32 \times 32 \times 3$ image and normalized for display.}
    \label{fig:loss_experiments_matrices}
\end{figure}

The results indicate that a high learning rate leads to unstable training, with validation accuracy dropping sharply, suggesting poor generalization and divergence. Conversely, a small learning rate ensures stable convergence, resulting in the highest test accuracy with smooth loss curves and effective generalization. Introducing moderate regularization, such as $\lambda=0.1$, slightly improves validation accuracy while keeping training accuracy nearly unchanged. However, heavy regularization, like $\lambda=1$, harms both training and generalization, indicating underfitting. These findings confirm that moderate regularization can enhance generalization, while excessive regularization reduces model capacity. Similarly, an appropriately small learning rate ensures convergence, whereas a large one destabilizes training.


\subsection{Improving performance}

We improved the performance of the network by implementing the following techniques:
\begin{itemize}
    \item More data samples: We increased the number of training samples by training on the entire CIFAR-10 dataset instead of a subset. This allowed the model to learn from a larger variety of examples, improving its generalization capabilities.
    \item Data augmentation: We applied random transformations to the training images, such as horizontal flipping to increase the diversity of the training set and reduce overfitting.
    \item GridSearch: We performed a grid search over the hyperparameters, including learning rate ($\eta$), batch size, and L2 regularization strength ($\lambda$). This helped us find the optimal combination of hyperparameters for our model:
    \begin{verbatim}
    ########### 3. Grid Search ###########
    n_subsamples = 1000
    # Define the grid search parameters
    eta_values = [0.001, 0.01, 0.1, 1]
    n_epochs_values = [40]
    n_batch_values = [50, 100, 200, 400]
    lam_values = [0, 0.1, 1]
    \end{verbatim}
\end{itemize}


\subsection{Train network with multiple binary cross-entropy losses}

These will be implemented next to complete the assignment fully.

\section{Conclusion}


\end{document}

